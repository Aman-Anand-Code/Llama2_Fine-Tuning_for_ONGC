# -*- coding: utf-8 -*-
"""PetrolemDatasetExtraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-wDNwsL5-OB4J2lIAspH3WNT-KubcODX
"""

import json
import random
import re
import nltk
from nltk.tokenize import sent_tokenize
from textblob import TextBlob

# Ensure necessary nltk data is downloaded
nltk.download('punkt')
# Download the 'punkt_tab' resource
nltk.download('punkt_tab')
# Download the 'brown' resource
nltk.download('brown') # Download the brown corpus

!pip install -U textblob # Update TextBlob

# Load the text file
with open("Oil and gas production handbook ed3x0_web.txt", "r", encoding="utf-8") as file:
    text = file.read()

# Split the text into sentences
sentences = sent_tokenize(text)

# Function to clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'\n+', ' ', text)  # Remove newlines
    return text.strip()

# Function to generate instructions
def generate_instruction(sentence):
    blob = TextBlob(sentence)
    # Check if noun_phrases is not empty before accessing elements
    if blob.noun_phrases:
        questions = [
            f"What is {blob.noun_phrases[0]}?",
            f"Describe the process of {blob.noun_phrases[0]}",
            f"How does {blob.noun_phrases[0]} work?",
            f"What are the benefits of {blob.noun_phrases[0]}?",
        ]
    else:
        # Provide a fallback instruction if no noun phrases are found
        questions = [f"Explain: {sentence}"]
    return random.choice(questions)


# Generate 1,500 instruction-response pairs
dataset = []
for sentence in sentences:
    clean_sentence = clean_text(sentence)
    if len(clean_sentence.split()) < 8:  # Skip very short sentences
        continue

    instruction = generate_instruction(clean_sentence)
    response = clean_sentence

    dataset.append({
        "system": "You are an expert in oil and gas production.",
        "instruction": instruction,
        "response": response
    })

    if len(dataset) >= 5000:  # Stop when we reach 1,500 rows
        break

# Save dataset as JSON
with open("llama2_finetune_5kRow.json", "w", encoding="utf-8") as json_file:
    json.dump(dataset, json_file, indent=4)

print("✅ Dataset with 5,000 instruction-response pairs generated and saved as 'llama2_finetune.json'")

import json
import re
import random
from textblob import TextBlob

# Load the dataset
with open("llama2_finetune_5kRow.json", "r", encoding="utf-8") as file:
    data = json.load(file)

# Define a function to clean and fix text
def clean_text(text):
    """Removes unwanted metadata, special characters, and book-related text."""
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces and newlines
    text = re.sub(r'\b(ISBN|copyright|©|edition|photo|wikimedia|statoil|ABB).*?\b', '', text, flags=re.IGNORECASE)  # Remove metadata
    return text.strip()

# Define a function to fix unclear instructions
def fix_instruction(instruction):
    """Improves the clarity of instructions using NLP and restructuring."""
    blob = TextBlob(instruction)
    # If the instruction is too vague, reformulate it
    if len(blob.words) < 3 or "?" not in instruction:
        return f"Can you explain {instruction.lower()} in detail?"

    return str(blob.correct())  # Auto-correct grammar issues

# Clean dataset
cleaned_data = []
for entry in data:
    system = entry["system"]
    instruction = fix_instruction(entry["instruction"])
    response = clean_text(entry["response"])

    # Filter out bad data (too short or meaningless)
    if len(instruction.split()) < 3 or len(response.split()) < 5:
        continue

    cleaned_data.append({
        "system": system,
        "instruction": instruction,
        "response": response
    })

# Save the cleaned dataset
with open("llama2_finetune_cleaned.json", "w", encoding="utf-8") as outfile:
    json.dump(cleaned_data, outfile, indent=4)

print(f"✅ Cleaned dataset saved as 'llama2_finetune_cleaned.json' with {len(cleaned_data)} high-quality examples.")