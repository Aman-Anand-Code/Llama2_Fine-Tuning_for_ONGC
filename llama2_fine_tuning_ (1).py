# -*- coding: utf-8 -*-
"""Llama2_Fine_Tuning .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_l4HsQvPdQ43-W10DgeyJ0P_KNv61Cm

# Fine Tuning Llama2

## Importing all the nessery library
"""

!pip install -q -U transformers peft accelerate bitsandbytes datasets

# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import Dataset
import torch

# Authenticate
from huggingface_hub import notebook_login
notebook_login()

"""## Loading my Custom data set on which fibe tuning has been done"""

# Load your dataset (from the provided JSON)
import json
with open('llama2_finetune_fully_cleaned.json') as f:
    data = json.load(f)

# Convert to Hugging Face Dataset format
dataset = Dataset.from_list(data)

"""## Loadin Llama 2 model"""

# Model setup - using 4-bit quantization to fit in T4 memory
model_name = "meta-llama/Llama-2-7b-chat-hf"

# Configure 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Load model with token from authentication
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# Prepare for QLoRA
model = prepare_model_for_kbit_training(model)

# LoRA config
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, peft_config)

"""## Preprocessing the data set"""

# Preprocessing - fixed to return dictionaries with correct structure
def preprocess_function(examples):
    model_inputs = {
        'input_ids': [],
        'attention_mask': [],
        'labels': []
    }

    for i in range(len(examples['system'])):
        text = f"<s>[INST] <<SYS>>\n{examples['system'][i]}\n<</SYS>>\n\n{examples['instruction'][i]} [/INST] {examples['response'][i]} </s>"
        tokenized = tokenizer(text, truncation=True, max_length=512, padding="max_length")

        model_inputs['input_ids'].append(tokenized['input_ids'])
        model_inputs['attention_mask'].append(tokenized['attention_mask'])
        model_inputs['labels'].append(tokenized['input_ids'])  # For causal LM, labels are same as input_ids

    return model_inputs

# Apply preprocessing
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset.column_names
)

# Custom data collator
def data_collator(features):
    batch = {
        'input_ids': torch.tensor([f['input_ids'] for f in features], dtype=torch.long),
        'attention_mask': torch.tensor([f['attention_mask'] for f in features], dtype=torch.long),
        'labels': torch.tensor([f['labels'] for f in features], dtype=torch.long)
    }
    return batch

"""## Training the llama 2 model"""

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    optim="paged_adamw_8bit",
    learning_rate=2e-4,
    num_train_epochs=2,
    fp16=True,
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    report_to="none",
    label_names=["input_ids", "attention_mask", "labels"]  # Explicitly specify label names
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

# Start training
trainer.train()

"""## Saving the model"""

# Save the adapter
model.save_pretrained("llama2_oil_gas_adapter")
tokenizer.save_pretrained("llama2_oil_gas_adapter")

"""## Merge the Trained model with base model and testing it."""

from peft import PeftModel
from torch.utils.data import DataLoader

print("\nTraining complete! Testing the fine-tuned model...\n")

# Reload the model for inference (to clear any training states)
# Clear memory

# del model, base_model, trainer
# torch.cuda.empty_cache()

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# Load fine-tuned adapter
model = PeftModel.from_pretrained(base_model, "llama2_oil_gas_adapter")

# Merge adapter with base model for better inference performance
model = model.merge_and_unload()

# Function to generate responses
def generate_response(system_prompt, user_prompt, max_new_tokens=200):
    # Format prompt according to Llama 2 chat template
    prompt = f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{user_prompt} [/INST]"

    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate response
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        do_sample=True
    )

    # Decode output
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract just the assistant's response
    response = response.split("[/INST]")[-1].strip()
    return response

# Example 1: Upstream process
system_prompt = "You are an expert in oil and gas production."
user_prompt = "Can you explain the upstream process in detail?"
print("=== Example 1: Upstream Process ===")
print(f"System: {system_prompt}")
print(f"User: {user_prompt}")
print("Model Response:")
print(generate_response(system_prompt, user_prompt))
print("\n" + "="*50 + "\n")

# Example 2: Specific technology question
system_prompt = "You are an oil and gas engineer with expertise in drilling technologies."
user_prompt = "What are the different types of offshore drilling platforms and their advantages?"
print("=== Example 2: Offshore Drilling Platforms ===")
print(f"System: {system_prompt}")
print(f"User: {user_prompt}")
print("Model Response:")
print(generate_response(system_prompt, user_prompt))
print("\n" + "="*50 + "\n")

# Example 3: From your training data
system_prompt = "You are an expert in oil and gas production."
user_prompt = "Now does refining work?"
print("=== Example 3: Refining (from training data) ===")
print(f"System: {system_prompt}")
print(f"User: {user_prompt}")
print("Model Response:")
print(generate_response(system_prompt, user_prompt))

"""## Downloading the model in the devise"""

from google.colab import files
import shutil

# Compress the directory
shutil.make_archive('llama2_oil_gas_adapter', 'zip', 'llama2_oil_gas_adapter')

# Download the zip file
files.download('llama2_oil_gas_adapter.zip')